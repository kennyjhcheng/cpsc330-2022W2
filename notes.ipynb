{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11-ensembles\n",
    "\n",
    "> **Use `scikit-learn`’s `RandomForestClassifier` and explain its main hyperparameters.**\n",
    "> \n",
    "\n",
    "```python\n",
    "pipe_rf = make_pipeline(\n",
    "    preprocessor, RandomForestClassifier(random_state=123, n_jobs=-1)\n",
    ")\n",
    "```\n",
    "\n",
    "- `n_estimators`: number of decision trees (higher = **increases complexity**)\n",
    "- `max_depth`: max depth of each decision tree (higher = **increases complexity**)\n",
    "- `max_features`: the number of features you get to look at each split (higher = **increases complexity**)\n",
    "- setting `random_state` is important for reproducibility\n",
    "- **Explain randomness in random forest algorithm.**\n",
    "    - **************************************************************************randomness in classifier construction**************************************************************************\n",
    "        1. **********Data:********** tree built on bootstrap sample (with replacement)\n",
    "        2. ****************Feature:**************** each node select **************************************************random subset of features************************************************** + best possible test involving themxdisplay\n",
    "\n",
    "> **Use other tree-based models such as as `XGBoost` and `LGBM`.**\n",
    "> \n",
    "\n",
    "```python\n",
    "pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123))\n",
    "pipe_xgb = make_pipeline(\n",
    "    preprocessor, XGBClassifier(random_state=123, eval_metric=\"logloss\", verbosity=0)\n",
    ")\n",
    "```\n",
    "\n",
    "> **Employ ensemble classifier approaches, in particular model averaging and stacking.\n",
    "\n",
    "Use `scikit-learn` implementations of these ensemble methods.**\n",
    "> \n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "classifiers = {\n",
    "    \"logistic regression\": pipe_lr,\n",
    "    \"decision tree\": pipe_dt,\n",
    "    \"random forest\": pipe_rf,\n",
    "    #\"XGBoost\": pipe_xgb,\n",
    "    \"LightGBM\": pipe_lgbm,\n",
    "    \"CatBoost\": pipe_catboost,\n",
    "}\n",
    "averaging_model = VotingClassifier(\n",
    "    list(classifiers.items()), voting=\"soft\"\n",
    ")  # need the list() here for cross_val to work!\n",
    "\n",
    "averaging_model.fit(X_train, y_train);\n",
    "```\n",
    "\n",
    "- `voting='hard'`\n",
    "    - output of `predict` and actually votes\n",
    "- `voting='soft'`\n",
    "    - averages output of `predict_proba` from base classifier, uses threshold/larger\n",
    "    - assumes we trust `predict_proba`\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "# remove cat boost for time\n",
    "classifiers_nocat = classifiers.copy()\n",
    "del classifiers_nocat[\"CatBoost\"]\n",
    "\n",
    "stacking_model = StackingClassifier(list(classifiers_nocat.items()))\n",
    "stacking_model.fit(X_train, y_train)\n",
    "\n",
    "pd.DataFrame(\n",
    "    data=stacking_model.final_estimator_.coef_[0],\n",
    "    index=classifiers_nocat.keys(),\n",
    "    columns=[\"Coefficient\"],\n",
    ")\n",
    "\n",
    "stacking_model.final_estimator_.intercept_\n",
    "```\n",
    "\n",
    "- default `final_estimator` is `LogisticRegression` for classification\n",
    "- does cross-validation by default\n",
    "    - fit base estimators on training fold → predicts on validation fold → fit meta-estimator on output (validation fold)\n",
    "    - `estimators_` fitted on full `X`\n",
    "    - `final_estimator_` trained using cross-validation predictions of base estimators using `cross_val_predict`\n",
    "\n",
    "> **Explain voting and stacking and the differences between them.**\n",
    "> \n",
    "\n",
    "| Voting | Stacking |\n",
    "| --- | --- |\n",
    "| Multiple models either soft vote (average) or hard vote to produce final classification | outputs of one model used as input to another |\n",
    "|  | outputs coefficients + intercept for each base classifier |\n",
    "| Takes long time to fit/predict | Takes very long time to fit/predict |\n",
    "| reduces interpretability |  |\n",
    "| reduces maintainability | reduces maintainability |\n",
    "|  | better accuracy generally than voting |\n",
    "- Voting\n",
    "    - Cons\n",
    "        - `fit` `predict` time\n",
    "        - reduces interpretability\n",
    "        - reduces maintainability\n",
    "- **Stacking:** one models output is input to another model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12 - Feature Importances\n",
    "\n",
    "> **Interpret the coefficients of linear regression for ordinal, one-hot encoded categorical, and scaled numeric features.**\n",
    "> \n",
    "\n",
    "```python\n",
    "lr = make_pipeline(preprocessor, Ridge())\n",
    "lr.fit(X_train, y_train)\n",
    "lr.named_steps['ridge'].coef_\n",
    "```\n",
    "\n",
    "- **********************************Ordinal features:**********************************\n",
    "    - easier to interpret\n",
    "    - increasing by one “ordered” category increases prediction by coefficient value\n",
    "- ************************Categorical:************************ use a ************************************reference category************************************\n",
    "    - subtracting all OHE coefficients of a category by one of the category coefficients\n",
    "    - coefficients explain difference between reference category and others\n",
    "- ********************************Scaled Numeric Features:******************************** Increase feature by 1 scaled unit changes prediction by coefficient value\n",
    "    - careful of scale when interpreting coefficients\n",
    "- coefficients tell about the ******************************************model, not accurately reflecting data******************************************\n",
    "\n",
    "> **Explain why interpretability is important in ML.**\n",
    "> \n",
    "- diagnosing errors in ML systems\n",
    "- not mindlessly trusting model with high accuracy\n",
    "- reasoning about predictions\n",
    "\n",
    "> **Use `feature_importances_` attribute of `sklearn` models and interpret its output.**\n",
    "> \n",
    "\n",
    "```python\n",
    "pipe_dt = make_pipeline(preprocessor, DecisionTreeClassifier(max_depth=3))\n",
    "pipe_dt.fit(X_train, y_train)\n",
    "pipe_dt.named_steps[\"decisiontreeclassifier\"].feature_importances_,\n",
    "```\n",
    "\n",
    "- `feature_importances_` don’t have sign\n",
    "    - ********************increasing******************** feature may cause prediction to go **up, then down**\n",
    "\n",
    "> **Use `eli5` to get feature importances of non `sklearn` models and interpret its output.**\n",
    "> \n",
    "\n",
    "```python\n",
    "conda install -c conda-forge eli5\n",
    "import eli5\n",
    "\n",
    "#LightGBM\n",
    "pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123))\n",
    "pipe_lgbm.fit(X_train, y_train)\n",
    "eli5.explain_weights(\n",
    "    pipe_lgbm.named_steps[\"lgbmclassifier\"], \n",
    "    feature_names=feature_names\n",
    ")\n",
    "```\n",
    "\n",
    "![Untitled](./img-notes/12-eli5.jpg)\n",
    "\n",
    "- tell us globally what features are important\n",
    "\n",
    "> **Apply SHAP to assess feature importances and interpret model predictions.**\n",
    "> \n",
    "- ******************************Shapley values:****************************** for each example & feature → explain prediction by computing contribution of each feature to prediction\n",
    "- For tree-based models\n",
    "    \n",
    "    ```python\n",
    "    import shap\n",
    "    \n",
    "    pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123))\n",
    "    pipe_lgbm.fit(X_train, y_train)\n",
    "    \n",
    "    lgbm_explainer = shap.TreeExplainer(pipe_lgbm.named_steps[\"lgbmclassifier\"])\n",
    "    # for each example & each feature\n",
    "    train_lgbm_shap_values = lgbm_explainer.shap_values(X_train_enc)\n",
    "    ```\n",
    "    \n",
    "\n",
    "> **Explain force plot, summary plot, and dependence plot produced with shapely values.**\n",
    "> \n",
    "- **Force plot**\n",
    "    \n",
    "    ```python\n",
    "    ex_l50k_index = # index of one value that is classified as <50k salary\n",
    "    shap.force_plot(\n",
    "        lgbm_explainer.expected_value[1],\n",
    "        test_lgbm_shap_values[1][ex_l50k_index, :],\n",
    "        X_test_enc.iloc[ex_l50k_index, :],\n",
    "        matplotlib=True,\n",
    "    )\n",
    "    ```\n",
    "    \n",
    "    ![Untitled](./img-notes/12-force.jpg)\n",
    "    \n",
    "    - from ********************base value******************** → average over dataset\n",
    "        - red → features pushing prediction towards higher score\n",
    "        - blue → features pushign prediction towards lower score\n",
    "    - feature importances sum to prediction\n",
    "- ************************Summary plot************************\n",
    "    \n",
    "    ```python\n",
    "    shap.summary_plot(train_lgbm_shap_values[1], X_train_enc, plot_type=\"bar\")\n",
    "    \n",
    "    shap.summary_plot(train_lgbm_shap_values[1], X_train_enc)\n",
    "    ```\n",
    "    \n",
    "\n",
    "![Untitled](./img-notes/12-summary1.jpg)\n",
    "\n",
    "![Untitled](./img-notes/12-summary2.jpg)\n",
    "\n",
    "- `married-civ-spouse` = bigger SHAP values for class 1\n",
    "- higher education = bigger SHAP\n",
    "- ******************************Dependency Plot******************************\n",
    "    \n",
    "    ```python\n",
    "    shap.dependence_plot(\"age\", train_lgbm_shap_values[1], X_train_enc)\n",
    "    ```\n",
    "    \n",
    "    ![Untitled](./img-notes/12-dependence.jpg)\n",
    "    \n",
    "    - X-axis = scaled `age` values\n",
    "    - Y-axis = SHAP values\n",
    "    - smaller age = smaller SHAP values\n",
    "    - optimal age value with highest SHAP around (scaled) `"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - K-Means Clustering\n",
    "\n",
    "> **Explain the unsupervised paradigm.**\n",
    "> \n",
    "- train model to find **patterns** in dataset that is typically ********unlabeled********\n",
    "\n",
    "> **Explain the motivation and potential applications of clustering.**\n",
    "> \n",
    "- Partition data into groups called clusters to ****************************************************discover underlying groups****************************************************\n",
    "    - labels arbitrarily identify clusters\n",
    "    - meaning depends on application and prior knowledge about data\n",
    "- Applications\n",
    "    1. ********************************Data exploration********************************\n",
    "    2. ******************************************Customer segmentation******************************************\n",
    "    3. **************************************Document clustering**************************************\n",
    "\n",
    "> **Define the clustering problem**\n",
    "> \n",
    "\n",
    "> **Broadly explain the K-Means algorithm.**\n",
    "> \n",
    "- Input: `X` data points, `K` clusters\n",
    "- initialization of `K` cluster centers\n",
    "- Iterate\n",
    "    1. assign example to closest center\n",
    "    2. estimate new center as **********************************************average of observations**********************************************\n",
    "- may not converge or converge sub-optimally\n",
    "\n",
    "> **Apply `sklearn`’s `KMeans` algorithm.**\n",
    "> \n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)\n",
    "# We are only passing X because this is unsupervised learning\n",
    "\n",
    "kmeans.predict(X)\n",
    "\n",
    "kmeans.cluster_centers_\n",
    "```\n",
    "\n",
    "> **Point out pros and cons of K-Means and the difficulties associated with choosing the right number of clusters.**\n",
    "> \n",
    "\n",
    "Pros\n",
    "\n",
    "- allows clustering\n",
    "- simple to understand, easy to implement\n",
    "- fast + scales to large data\n",
    "- choose clusters using elbow and silhouette method\n",
    "\n",
    "Cons\n",
    "\n",
    "- Stochastic initialization → can start poorly\n",
    "- may converge suboptimally\n",
    "- must specify clusters in advance\n",
    "- each example must be assigned to one cluster\n",
    "\n",
    "> **Create the Elbow plot and Silhouette plots for a given dataset.**\n",
    "> \n",
    "- **************Elbow Method:************** looks at **************inertia**************, the sum of **********************************************************intra-cluster distances********************************************************** between points and their cluster center\n",
    "    \n",
    "    ```python\n",
    "    from yellowbrick.cluster import KElbowVisualizer\n",
    "    \n",
    "    model = KMeans()\n",
    "    visualizer = KElbowVisualizer(model, k=(1, 10))\n",
    "    \n",
    "    visualizer.fit(X)  # Fit the data to the visualizer\n",
    "    visualizer.finalize()\n",
    "    \n",
    "    visualizer.draw()\n",
    "    ```\n",
    "    \n",
    "    ![Untitled](./img-notes/13-elbow.jpg)\n",
    "    \n",
    "    - elbow at `k=3` → more clusters doesn’t bring improvement in decreasing inertia\n",
    "- ********************************Silhouette plot:******************************** calculated using\n",
    "    - **mean intra-cluster distance $a$:** distance from a point to other points in same cluster\n",
    "    - **mean nearest-cluster distance $b$:** distance from point to other points in nearest cluster\n",
    "    - ****************************************silhouette distance:**************************************** difference between $b-a$ normalized by maximum value\n",
    "        - $[-1, 1]$\n",
    "        - $0$ means overlapping clusters\n",
    "    - **********************************Silhouette score:********************************** average of silhouette score for all samples\n",
    "    \n",
    "    ```python\n",
    "    from yellowbrick.cluster import SilhouetteVisualizer\n",
    "    ```\n",
    "    \n",
    "\n",
    "```python\n",
    "model = KMeans(2, random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show();\n",
    "# Finalize and render the figure\n",
    "```\n",
    "\n",
    "```python\n",
    "model = KMeans(3, random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show();\n",
    "# Finalize and render the figure\n",
    "```\n",
    "\n",
    "```python\n",
    "model = KMeans(5, random_state=42)\n",
    "visualizer = SilhouetteVisualizer(model, colors=\"yellowbrick\")\n",
    "visualizer.fit(X)  # Fit the data to the visualizer\n",
    "visualizer.show();\n",
    "# Finalize and render the figure\n",
    "```\n",
    "\n",
    "![Untitled](./img-notes/13-silhouette2.jpg)\n",
    "\n",
    "![Untitled](./img-notes/13-silhouette3.jpg)\n",
    "\n",
    "![Untitled](./img-notes/13-silhouette5.jpg)\n",
    "\n",
    "- Silhouette score for each sample in cluster\n",
    "    - ********************************higher value →******************************** well-separated\n",
    "    - size → # samples\n",
    "- ****************************************more rectangular →**************************************** points are happy in cluster\n",
    "\n",
    "> **Visualize clusters in low dimensional space.**\n",
    "> \n",
    "\n",
    "```python\n",
    "import umap\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(data_df)\n",
    "\n",
    "reducer = umap.UMAP(n_neighbors=15)\n",
    "Z = reducer.fit_transform(data_df)\n",
    "umap_df = pd.DataFrame(data=Z, columns=[\"dim1\", \"dim2\"])\n",
    "umap_df[\"cluster\"] = kmeans.labels_\n",
    "\n",
    "labels = np.unique(umap_df[\"cluster\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "ax.set_title(\"K-means with k = 3\")\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    umap_df[\"dim1\"],\n",
    "    umap_df[\"dim2\"],\n",
    "    c=umap_df[\"cluster\"],\n",
    "    cmap=\"tab20b\",\n",
    "    s=50,\n",
    "    edgecolors=\"k\",\n",
    "    linewidths=0.1,\n",
    ")\n",
    "\n",
    "legend = ax.legend(*scatter.legend_elements(), loc=\"best\", title=\"Clusters\")\n",
    "ax.add_artist(legend)\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "![k=3](./img-notes/13-lowdim.jpg)\n",
    "\n",
    "k=3\n",
    "\n",
    "> **Use clustering for customer segmentation problem.**\n",
    "> \n",
    "- ********************************************Customer segmentation:******************************************** understand landscape of market in business to tailor products to each group\n",
    "    - uses demographic, geographic, psychographic, behavioral data\n",
    "1. Preprocess, EDA, and build pipeline\n",
    "2. analyze which `k` to use using elbow or silhouette plots\n",
    "3. build model\n",
    "    \n",
    "    ```python\n",
    "    kmeans = KMeans(n_clusters=4, random_state=42)\n",
    "    kmeans.fit(transformed_df)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    cluster_centers = pd.DataFrame(\n",
    "        data=kmeans.cluster_centers_, columns=[transformed_df.columns]\n",
    "    )\n",
    "    cluster_centers\n",
    "    ```\n",
    "    \n",
    "4. ********************************************************************************************inverse transform to unscale cluster centers********************************************************************************************\n",
    "    \n",
    "    ```python\n",
    "    data = (\n",
    "        preprocessor.named_transformers_[\"pipeline\"]\n",
    "        .named_steps[\"standardscaler\"]\n",
    "        .inverse_transform(cluster_centers[numeric_features])\n",
    "    )\n",
    "    ```\n",
    "    \n",
    "\n",
    "> **Interpret the clusters discovered by K-Means.**\n",
    ">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 - DBSCAN & Hierchical Clustering\n",
    "\n",
    "> **Identify limitations of K-Means.**\n",
    "> \n",
    "- Stochastic initialization → can start poorly\n",
    "- may converge suboptimally\n",
    "- must specify clusters in advance\n",
    "- each example must be assigned to one cluster\n",
    "- ********************************************************fails to identify complexity******************************************************** in shape of data\n",
    "    - ******************************************boundaries are linear******************************************\n",
    "\n",
    "> **Explain the difference between core points, border points, and noise points in the context of DBSCAN.**\n",
    "> \n",
    "- Iterative algorithm using 3 kinds of points to identify dense regions\n",
    "    1. **********************Core point:********************** points with `min_samples` points within `eps` distance\n",
    "    2. **************************Border point:************************** connected to core point within `eps` distance, but fewer than `min_samples`\n",
    "    3. ************************Noise point:************************ points don’t belong to cluster \n",
    "\n",
    "> **Broadly explain how DBSCAN works.**\n",
    "> \n",
    "- ****************DBSCAN:**************** Density-Based Spatial Clustering of Applications with Noise\n",
    "    - ****************************************************identify crowded regions:**************************************************** clusters form dense regions in data\n",
    "- algorithm\n",
    "    1. pick random point $p$\n",
    "    2. check whether $p$ is ********************core point******************** (at least `min_samples` neighbors within `eps`)\n",
    "    3. if core point, give label\n",
    "    4. for neighbors of $p$, check if ********************core point******************** → continue spreading label if core point\n",
    "    5. once no more core points to spread label, pick new unlabelled $p$ and repeat\n",
    "\n",
    "> **Apply DBSCAN using `sklearn`.**\n",
    "> \n",
    "\n",
    "```python\n",
    "dbscan = DBSCAN(eps=0.2)\n",
    "dbscan.fit(X)\n",
    "```\n",
    "\n",
    "> **Explain the effect of epsilon and minimum samples hyperparameters in DBSCAN.**\n",
    "> \n",
    "- `eps` : determines ******************closeness****************** of points\n",
    "    - small eps → difficult to find neighbors\n",
    "    - good eps → detects neighbors forming some clusters\n",
    "    - high eps → points all in one cluster\n",
    "- `min_samples` : determines # neighboring points to consider as part of cluster\n",
    "    - low min_samples → few outliers, many clusters\n",
    "    - good min_samples → some outliers, fewer clusters\n",
    "    - high min_samples → many outliers, very few clusters\n",
    "\n",
    "> **Identify DBSCAN limitations.**\n",
    "> \n",
    "\n",
    " Pros\n",
    "\n",
    "- doesn’t require # clusters in advance\n",
    "- identifies points not part of any cluster\n",
    "- captures complex shapes\n",
    "- use silhouette method\n",
    "\n",
    "Cons\n",
    "\n",
    "- must tune hyperparameter `esp` and `min_samples`\n",
    "- doesn’t `predict` new points (only existing)\n",
    "- cannot use elbow method\n",
    "- fails for varying density\n",
    "\n",
    "> **Explain the idea of hierarchical clustering.**\n",
    "> \n",
    "- get picture of similarity before picking # clusters\n",
    "- ******************Algorithm******************\n",
    "    1. start with every point in own cluster\n",
    "    2. ****************************greedily merge**************************** similar clusters\n",
    "    3. repeat until only one cluster ($n-1$ times)\n",
    "\n",
    "> **Visualize dendrograms using `scipy.cluster.hierarchy.dendrogram`.**\n",
    "> \n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "ax = plt.gca()\n",
    "dendrogram(linkage_array, ax=ax)\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\");\n",
    "```\n",
    "\n",
    "- `truncate_mode` to control dendrogram length\n",
    "    - `lastp` = # leaves\n",
    "    - `level` = max depth\n",
    "    \n",
    "    ```python\n",
    "    dendrogram(Z, p=6, truncate_mode=\"lastp\", ax=ax, labels=data.index)\n",
    "    dendrogram(Z, p=6, truncate_mode=\"level\", ax=ax, labels=data.index);\n",
    "    ```\n",
    "    \n",
    "- `fcluster` to flatten\n",
    "    \n",
    "    ```python\n",
    "    from scipy.cluster.hierarchy import fcluster\n",
    "    \n",
    "    cluster_labels = fcluster(Z, 6, criterion=\"maxclust\")\n",
    "    \n",
    "    pd.DataFrame(cluster_labels, data.index)\n",
    "    ```\n",
    "    \n",
    "\n",
    "> **Explain the advantages and disadvantages of different clustering methods.**\n",
    "> \n",
    "\n",
    "|  | K-means | DBSCAN | hierarchical |\n",
    "| --- | --- | --- | --- |\n",
    "| Advantage | - easy implement\n",
    "- fast/efficient for large data\n",
    "- works well for linearly separated data\n",
    "- variety of distance metrics | - automatically identifies # clusters\n",
    "- identifies irregular shapes\n",
    "- robust to outliers | - hierarchy of clusters, range of solutions\n",
    "- doesn’t require specifying # clusters\n",
    "- distance metrics & linkage methods\n",
    "- identify clusters at varying granularity |\n",
    "| Disadvantage | - pre-specified # clusters\n",
    "- sensitive to initial selection → different results\n",
    "- can converge sub-optimally\n",
    "- not suitable for irregular shaped data | - sensitivity to hyperparameters\n",
    "- computationally expensive for large data\n",
    "- requires tuning\n",
    "- doesn’t work for varying density | - computationally expensive for large data\n",
    "- sensitive to distance/linkage choice\n",
    "- difficult to determine optimal number clusters\n",
    "- suffer from chaining effect (merging too early) |\n",
    "\n",
    "> **Apply clustering algorithms on image datasets and interpret clusters.**\n",
    "> \n",
    "\n",
    "> **Recognize the impact of distance measure and representation in clustering methods.**\n",
    "> \n",
    "\n",
    "```python\n",
    "from scipy.cluster.hierarchy import (\n",
    "    average,\n",
    "    complete,\n",
    "    dendrogram,\n",
    "    fcluster,\n",
    "    single,\n",
    "    ward,\n",
    ")\n",
    "\n",
    "Z = single(X)\n",
    "Z = average(X)\n",
    "Z = complete(X)\n",
    "Z = ward(X)\n",
    "dendrogram(Z)\n",
    "```\n",
    "\n",
    "- similarity/********************************linkage criteria******************************** between clusters\n",
    "    1. ******************************single linkage:****************************** smallest, min-distance between two clusters\n",
    "    2. ********************************average linkage:******************************** smallest, avg-distance between two clusters\n",
    "    3. ******************************************complete/max linkage:****************************************** smallest, max-distance between points of two clusters\n",
    "    4. **************************ward linkage:************************** merge clusters to minimize increase in cluster variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 - Recommender Systems\n",
    "\n",
    "> **State the problem of recommender systems.**\n",
    "> \n",
    "- **************************************Recommender system:************************************** Recommend particular product/service to users that are likely to consume\n",
    "    - requires user ratings, features related to items + users, customer purchase history\n",
    "- **************ratings************** for set of $M$ items for $N$ users\n",
    "\n",
    "> **Describe components of a utility matrix.**\n",
    "> \n",
    "- ******************************Utility matrix:****************************** iteration between $N$ users, $M$ items\n",
    "    - e.g. ratings, clicks, purchases\n",
    "    - complete utility matrix to recommend items they will rate higher\n",
    "\n",
    "> **Create a utility matrix given ratings data.**\n",
    "> \n",
    "\n",
    "```python\n",
    "# Maps user/item to a number\n",
    "user_mapper = dict(zip(np.unique(ratings[user_key]), list(range(N))))\n",
    "item_mapper = dict(zip(np.unique(ratings[item_key]), list(range(M))))\n",
    "# Maps number to user/item\n",
    "user_inverse_mapper = dict(zip(list(range(N)), np.unique(ratings[user_key])))\n",
    "item_inverse_mapper = dict(zip(list(range(M)), np.unique(ratings[item_key])))\n",
    "\n",
    "user_key = \"userId\"\n",
    "item_key = \"productId\"\n",
    "Y = np.zeros((N, M))\n",
    "Y.fill(np.nan)\n",
    "for index, val in data.iterrows():\n",
    "    n = user_mapper[val[user_key]]\n",
    "    m = item_mapper[val[item_key]]\n",
    "    Y[n, m] = val[\"rating\"]\n",
    "```\n",
    "\n",
    "> **Describe a common approach to evaluate recommender systems.**\n",
    "> \n",
    "- ********************Diversity:******************** how different are recommendations\n",
    "- ********************Freshness:******************** people like new/surprising things, tradeoff is trust issues and explaining recommendations\n",
    "- ************************Persistence:************************ how long recommendation lasts\n",
    "- ********************************************Social recommendation:******************************************** what did friends watch\n",
    "\n",
    "> **Implement some baseline approaches to complete the utility matrix.**\n",
    "> \n",
    "\n",
    "```python\n",
    "X = ratings.copy()\n",
    "# will not use y\n",
    "y = ratings[user_key]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_valid.shape\n",
    "# ((113089, 3), (28273, 3))\n",
    "\n",
    "# Create training and validation utility matrices\n",
    "train_mat = create_Y_from_ratings(X_train, N, M, user_mapper, item_mapper)\n",
    "valid_mat = create_Y_from_ratings(X_valid, N, M, user_mapper, item_mapper)\n",
    "train_mat.shape, valid_mat.shape\n",
    "# ((3635, 140), (3635, 140))\n",
    "# same shape, but each only contain a proportion of all ratings\n",
    "```\n",
    "\n",
    "```python\n",
    "def error(X1, X2):\n",
    "    \"\"\"\n",
    "    Returns the root mean squared error.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.nanmean((X1 - X2) ** 2))\n",
    "\n",
    "def evaluate(pred_X, train_X, valid_X, model_name=\"Global average\"):\n",
    "    print(\"%s train RMSE: %0.2f\" % (model_name, error(pred_X, train_X)))\n",
    "    print(\"%s valid RMSE: %0.2f\" % (model_name, error(pred_X, valid_X)))\n",
    "```\n",
    "\n",
    "- predict as global average\n",
    "    \n",
    "    ```python\n",
    "    avg = np.nanmean(train_mat)\n",
    "    pred_g = np.zeros(train_mat.shape) + avg\n",
    "    \n",
    "    evaluate(pred_g, train_mat, valid_mat, model_name=\"Global average\")\n",
    "    '''\n",
    "    Global average train RMSE: 5.75\n",
    "    Global average valid RMSE: 5.77\n",
    "    '''\n",
    "    ```\n",
    "    \n",
    "- KNN imputation: imputate using ********************mean value******************** of kNN in training set, distances between existing values\n",
    "    \n",
    "    ```python\n",
    "    from sklearn.impute import KNNImputer\n",
    "    \n",
    "    imputer = KNNImputer(n_neighbors=10)\n",
    "    train_mat_imp = imputer.fit_transform(train_mat)\n",
    "    \n",
    "    evaluate(train_mat_imp, train_mat, valid_mat, model_name=\"KNN imputer\")\n",
    "    '''\n",
    "    KNN imputer train RMSE: 0.00\n",
    "    KNN imputer valid RMSE: 4.79\n",
    "    '''\n",
    "    ```\n",
    "    \n",
    "\n",
    "> **Explain the idea of collaborative filtering.**\n",
    "> \n",
    "- ****************************************************unsupervised learning →**************************************************** learn features using sparse labels\n",
    "- **intuition**: similar users and items help predict entries → leveraging ************************************social information************************************\n",
    "- can use ********************************cross-validation******************************** and ************grid search************\n",
    "\n",
    "```python\n",
    "import surprise\n",
    "from surprise import SVD, Dataset, Reader, accuracy\n",
    "\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(ratings, reader)  # Load the data\n",
    "\n",
    "# I'm being sloppy here. Probably there is a way to create validset from our already split data.\n",
    "trainset, validset = surprise.model_selection.train_test_split(\n",
    "    data, test_size=0.2, random_state=42\n",
    ")  # Split the data\n",
    "\n",
    "k = 10\n",
    "algo = SVD(n_factors=k, random_state=42)\n",
    "algo.fit(trainset)\n",
    "svd_preds = algo.test(validset)\n",
    "accuracy.rmse(svd_preds, verbose=True)\n",
    "\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "pd.DataFrame(cross_validate(algo, data, measures=[\"RMSE\", \"MAE\"], cv=5, verbose=True))\n",
    "```\n",
    "\n",
    "> **Explain the idea of content-based filtering.**\n",
    "> \n",
    "- e.g. for movie recommendation\n",
    "    - we know movie ratings\n",
    "    - we know movie features\n",
    "    - we can create profile for each user based on the movies they like\n",
    "- rating prediction → **********************************************************supervised regressoin problem**********************************************************\n",
    "    - given movie info, create profile\n",
    "    - build regression model for each user, learn regression weights\n",
    "    - each user has personalized regression model\n",
    "\n",
    "********Pros********\n",
    "\n",
    "- don’t need many users to provide rating\n",
    "- each user modeled separately → uniqueness of taste\n",
    "- can obtain **features of items**, can immediately recommend new items\n",
    "    - not possible with collaborative filtering\n",
    "- recommendations **************************interpretable************************** by weights\n",
    "\n",
    "********Cons********\n",
    "\n",
    "- feature acquisition & feature engineering\n",
    "    - what features should we use to explain difference in ratings?\n",
    "    - obtaining features for each item may be expensive\n",
    "- ************************************less diversity →************************************ hardly recommend item outside profile\n",
    "- ****************************cold start →**************************** new users, no information\n",
    "\n",
    "> **Explain some serious consequences of recommendation systems.**\n",
    "> \n",
    "- User exposed to information **reinforcing beliefs and biases**\n",
    "    - increases **polarization**, **lessens diversity** in perspective\n",
    "- maximizing user engagement by **reinforcing harmful ideas**\n",
    "- perpetuate/amplify ******************************************bias & discrimination****************************************** by learning them and recommending the products/content\n",
    "- ************************************privacy violations************************************ by relying on personal user data without consent\n",
    "- **********************************************************misinformation and propoganda********************************************************** if recommendations spread false information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16 - NLP Intro\n",
    "\n",
    "> **Broadly explain what is natural language processing (NLP).**\n",
    "> \n",
    "- ******NLP:****** making computers understand what humans say\n",
    "    - requires common sense and reasoning\n",
    "        - ************************************lexical ambiguity:************************************ e.g. panini means a sandwich and a person’s name\n",
    "        - ********************************************referential ambiguity:******************************************** e.g. the word “it” could refer to multiple nouns in a sentence\n",
    "\n",
    "> **Name some common NLP applications.**\n",
    "> \n",
    "- Voice assistants\n",
    "- auto-complete\n",
    "- translation\n",
    "\n",
    "> **Explain the general idea of a vector space model.**\n",
    "> \n",
    "- represent text as a ********************************************************************************numeric vector in high-dimensional space********************************************************************************\n",
    "    - each word represented as a point\n",
    "    - distance between points represents ********************similarity********************\n",
    "        - i.e. normalized dot product between word vectors or **********************************cosine similarity**********************************\n",
    "\n",
    "> **Explain the difference between different `word representations`: term-term co-occurrence matrix representation and Word2Vec representation.**\n",
    "> \n",
    "\n",
    "| term-term co-occurrence matrix | Word2Vec representation. |\n",
    "| --- | --- |\n",
    "| in text, counts words within a context window | dense word embedding trained with ML models |\n",
    "| long, sparse representation to capture relationship between words | short, dense vectors |\n",
    "|  | training is expensive |\n",
    "|  | pre-trained word embedding |\n",
    "| BoW is document-term co-occurrence matrix |  |\n",
    "\n",
    "> **Describe the reasons and benefits of using pre-trained embeddings.**\n",
    "> \n",
    "- pre-computed word/phrase representations trained on large amount of data useful when\n",
    "    1. ************************Data scarce:************************ data available is limited\n",
    "    2. ****************************Time efficient**************************** to skip step of training own word embedding\n",
    "    3. ****************************************improved performance**************************************** with good initialization point to capture semantic and syntactic information to learn better representations downstream\n",
    "    4. ************************************transfer learning:************************************ model trained on one task fine-tuned on new task\n",
    "    5. ******************************************multilingual support******************************************\n",
    "\n",
    "> **Load and use pre-trained word embeddings to find word similarities and analogies.**\n",
    "> \n",
    "\n",
    "```python\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "\n",
    "google_news_vectors = api.load('word2vec-google-news-300')\n",
    "\n",
    "google_news_vectors.most_similar(\"UBC\")\n",
    "google_news_vectors.similarity(\"Japan\", \"hockey\")\n",
    "\n",
    "# analogy\n",
    "print(\"%s : %s :: %s : ?\" % (word1, word2, word3))\n",
    "sim_words = model.most_similar(positive=[word3, word2], negative=[word1])\n",
    "return pd.DataFrame(sim_words, columns=[\"Analogy word\", \"Score\"])\n",
    "```\n",
    "\n",
    "> **Demonstrate biases in embeddings and learn to watch out for such biases in pre-trained embeddings.**\n",
    "> \n",
    "\n",
    "> **Use word embeddings in text classification and document clustering using `spaCy`.**\n",
    "> \n",
    "\n",
    "```python\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "doc = nlp(\"pineapple\") # extract all interesting information about the document\n",
    "\n",
    "#Average Embedding\n",
    "doc = nlp(\"All empty promises\")\n",
    "avg_sent_emb = doc.vector\n",
    "\n",
    "# document similarity\n",
    "doc1 = nlp(\"Deep learning is very popular these days.\")\n",
    "doc2 = nlp(\"Machine learning is dominated by neural networks.\")\n",
    "doc3 = nlp(\"A home-made fresh bread with butter and cheese.\")\n",
    "doc1.similarity(doc2)\n",
    "```\n",
    "\n",
    "Airline sentiment analysis\n",
    "\n",
    "- Split data\n",
    "    \n",
    "    ```python\n",
    "    from sklearn.model_selection import cross_validate, train_test_split\n",
    "    \n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=123)\n",
    "    X_train, y_train = train_df[\"text\"], train_df[\"airline_sentiment\"]\n",
    "    X_test, y_test = test_df[\"text\"], test_df[\"airline_sentiment\"]\n",
    "    ```\n",
    "    \n",
    "- BoW representation\n",
    "    \n",
    "    ```python\n",
    "    pipe = make_pipeline(\n",
    "        CountVectorizer(stop_words=\"english\"), LogisticRegression(max_iter=1000)\n",
    "    )\n",
    "    pipe.named_steps[\"countvectorizer\"].fit(X_train)\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pipe.score(X_train, y_train)\n",
    "    pipe.score(X_test, y_test)\n",
    "    ```\n",
    "    \n",
    "- Average Embedding\n",
    "    \n",
    "    ```python\n",
    "    # get word vectors by creating average embedding representation for examples\n",
    "    X_train_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_train)])\n",
    "    X_test_embeddings = pd.DataFrame([text.vector for text in nlp.pipe(X_test)])\n",
    "    \n",
    "    # Logistic Regression\n",
    "    lgr = LogisticRegression(max_iter=2000)\n",
    "    lgr.fit(X_train_embeddings, y_train)\n",
    "    lgr.score(X_train_embeddings, y_train)\n",
    "    lgr.score(X_test_embeddings, y_test)\n",
    "    ```\n",
    "    \n",
    "\n",
    "> **Explain the general idea of topic modeling.**\n",
    "> \n",
    "- **********************************Topic modelling:********************************** summarize major themes in collection of documents (corpus)\n",
    "    - organize and categorize documents on variety of topics\n",
    "    - commonly using ****************************************unsupervised methods****************************************\n",
    "- application\n",
    "    - EDA to get sense of large corpus\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_topics = 3 # number of topics\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_topics, learning_method=\"batch\", max_iter=10, random_state=0\n",
    ")\n",
    "lda.fit(toy_X) \n",
    "# document-topic association\n",
    "document_topics = lda.transform(toy_X)\n",
    "\n",
    "# word-topic association: weight associated with each word\n",
    "lda.components_\n",
    "```\n",
    "\n",
    "> **Describe the input and output of topic modeling.**\n",
    "> \n",
    "- ************Input:************\n",
    "    - collection of documents\n",
    "    - hyperparameter for number of topics/clusters $K$\n",
    "- **************Output:**************\n",
    "    1. **********************************************Topic-word association:********************************************** for each topic, what words describe it\n",
    "    2. **************Document-topics association:************** what topic expressed by each document\n",
    "\n",
    "> **Carry out basic text preprocessing using `spaCy`.**\n",
    "> \n",
    "\n",
    "```python\n",
    "clean_text = []\n",
    "min_token_len = 2\n",
    "irrelevant_pos=[\"ADV\", \"PRON\", \"CCONJ\", \"PUNCT\", \"PART\", \"DET\", \"ADP\", \"SPACE\"]\n",
    "\n",
    "for token in nlp.pipe(text_df[\"text\"]):\n",
    "    if (\n",
    "        token.is_stop == False  # Check if it's not a stopword\n",
    "        and len(token) > min_token_len  # Check if the word meets minimum threshold\n",
    "        and token.pos_ not in irrelevant_pos\n",
    "    ):  # Check if the POS is in the acceptable POS tags\n",
    "        lemma = token.lemma_  # Take the lemma of the word\n",
    "        clean_text.append(lemma.lower())\n",
    "return \" \".join(clean_text)\n",
    "```\n",
    "\n",
    "- **************************tokenization:**************************\n",
    "    - sentence segmentation (text into sentences)\n",
    "        \n",
    "        ```python\n",
    "        from nltk.tokenize import sent_tokenize\n",
    "        sent_tokenized = sent_tokenize(text)\n",
    "        ```\n",
    "        \n",
    "    - word tokenization (sentences into words)\n",
    "        \n",
    "        ```python\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        \n",
    "        word_tokenized = [word_tokenize(sent) for sent in sent_tokenized]\n",
    "        ```\n",
    "        \n",
    "- Punctuation and stopword removal\n",
    "- lemmatization: convert inflected form of words into base form\n",
    "- stemming: chopping affixes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17 - Multi-class classification\n",
    "\n",
    "> **Apply classifiers to multi-class classification algorithms.**\n",
    "> \n",
    "1. **********************One vs Rest:********************** For each class, binary model separating it from all others (imbalanced) → scores from all binary classifiers determines winner\n",
    "    \n",
    "    ```python\n",
    "    lr = LogisticRegression(max_iter=2000, multi_class=\"ovr\")\n",
    "    ```\n",
    "    \n",
    "2. ********************One vs One:******************** binary model for each pair of classes, $\\frac{n\\times (n-1)}{2}$ binary classifiers → apply all classifiers, most votes wins\n",
    "- Wrappers for any binary classifier\n",
    "    - **`[OneVsRestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html)`**\n",
    "    - **`[OneVsOneClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html)`**\n",
    "    \n",
    "    ```python\n",
    "    model = OneVsOneClassifier(LogisticRegression())\n",
    "    ```\n",
    "    \n",
    "\n",
    "> **Explain the role of neural networks in machine learning, and the pros/cons of using them.**\n",
    "> \n",
    "- ********************************Neural networks:******************************** sequence of transformations on input data\n",
    "- **Perceptron**: single layer neural network with input/output layer and adjustable weights\n",
    "- ********************************************************Multi Layer Perceptron (MLP)********************************************************: multi-layers of perceptrons\n",
    "    - layers can apply non-linear functions\n",
    "    - can specify number of features after each transormation\n",
    "\n",
    "********Pros********\n",
    "\n",
    "- Learn **complex** functions\n",
    "- tradeoff controlls by # layers & layer size\n",
    "- more/bigger layers → more complexity\n",
    "- can get **model that won’t underfit**\n",
    "- Works well for **structured** data\n",
    "    - 1D sequences (e.g. timeseries, language)\n",
    "    2D image\n",
    "    - 3D image or video\n",
    "- **Transfer** **learning** is useful\n",
    "\n",
    "**Cons**\n",
    "\n",
    "- requires lots of **data**\n",
    "- requires high **compute time**, and GPUs to be faster\n",
    "- **huge** number of **hyperparameters** → difficult to tune\n",
    "    - each layer has hyperparameters + overall hyperparameters\n",
    "- **not interpretable**\n",
    "- `fit` **not** guaranteed to be **optimal**\n",
    "    - hyperparameters specific to `fit`\n",
    "    - don’t know if it was successful\n",
    "    - never know how long to run `fit`\n",
    "\n",
    "> **Explain why the methods we’ve learned previously would not be effective on image data.**\n",
    "> \n",
    "- Previous methods require flattening data for images into vector of features → ********************************************************************************removes structured information of images********************************************************************************\n",
    "- ********************************Convolutional neural networks (CNN):******************************** use images without flattening\n",
    "\n",
    "> **Apply pre-trained neural networks to classification and regression problems.**\n",
    "> \n",
    "\n",
    "```python\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.models import vgg16\n",
    "\n",
    "clf = vgg16(weights='VGG16_Weights.DEFAULT') \n",
    "preprocess = transforms.Compose([\n",
    "                 transforms.Resize(299),\n",
    "                 transforms.CenterCrop(299),\n",
    "                 transforms.ToTensor(),\n",
    "                 transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                     std=[0.229, 0.224, 0.225]),])\n",
    "\n",
    "img_t = preprocess(img)\n",
    "batch_t = torch.unsqueeze(img_t, 0)\n",
    "\n",
    "clf.eval()\n",
    "  output = clf(batch_t)\n",
    "  _, indices = torch.sort(output, descending=True)\n",
    "  probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "  d = {'Class': [classes[idx] for idx in indices[0][:topn]], \n",
    "       'Probability score': [np.round(probabilities[0, idx].item(),3) for idx in indices[0][:topn]]}\n",
    "  df = pd.DataFrame(d, columns = ['Class','Probability score'])\n",
    "```\n",
    "\n",
    "> **Utilize pre-trained networks as feature extractors and combine them with models we’ve learned previously.**\n",
    "> \n",
    "\n",
    "```python\n",
    "# Attribution: [Code from PyTorch docs](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html?highlight=transfer%20learning)\n",
    "\n",
    "IMAGE_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "data_transforms = {\n",
    "    \"train\": transforms.Compose(\n",
    "        [\n",
    "            # transforms.RandomResizedCrop(224),\n",
    "            # transforms.RandomHorizontalFlip(),\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),     \n",
    "            transforms.ToTensor(),\n",
    "            #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),            \n",
    "        ]\n",
    "    ),\n",
    "    \"valid\": transforms.Compose(\n",
    "        [\n",
    "            # transforms.Resize(256),\n",
    "            # transforms.CenterCrop(224),\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),                        \n",
    "            transforms.ToTensor(),\n",
    "            # transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),                        \n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "data_dir = \"data/animal-faces\"\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in [\"train\", \"valid\"]\n",
    "}\n",
    "dataloaders = {\n",
    "    x: torch.utils.data.DataLoader(\n",
    "        image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4\n",
    "    )\n",
    "    for x in [\"train\", \"valid\"]\n",
    "}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in [\"train\", \"valid\"]}\n",
    "class_names = image_datasets[\"train\"].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_features(model, train_loader, valid_loader):\n",
    "    \"\"\"Extract output of squeezenet model\"\"\"\n",
    "    with torch.no_grad():  # turn off computational graph stuff\n",
    "        Z_train = torch.empty((0, 1024))  # Initialize empty tensors\n",
    "        y_train = torch.empty((0))\n",
    "        Z_valid = torch.empty((0, 1024))\n",
    "        y_valid = torch.empty((0))\n",
    "        for X, y in train_loader:\n",
    "            Z_train = torch.cat((Z_train, model(X)), dim=0)\n",
    "            y_train = torch.cat((y_train, y))\n",
    "        for X, y in valid_loader:\n",
    "            Z_valid = torch.cat((Z_valid, model(X)), dim=0)\n",
    "            y_valid = torch.cat((y_valid, y))\n",
    "    return Z_train.detach(), y_train.detach(), Z_valid.detach(), y_valid.detach()\n",
    "\n",
    "densenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\n",
    "densenet.classifier = nn.Identity()  # remove that last \"classification\" layer\n",
    "\n",
    "Z_train, y_train, Z_valid, y_valid = get_features(\n",
    "    densenet, dataloaders[\"train\"], dataloaders[\"valid\"]\n",
    ")\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), LogisticRegression(max_iter=2000))\n",
    "pipe.fit(Z_train, y_train)\n",
    "pipe.score(Z_train, y_train)\n",
    "# 1.0\n",
    "pipe.score(Z_valid, y_valid)\n",
    "# 0.9\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18 - Time Series\n",
    "\n",
    "> **Recognize when it is appropriate to use time series.**\n",
    "> \n",
    "- Data indexed in time order\n",
    "\n",
    "> **Explain the pitfalls of train/test splitting with time series data.**\n",
    "> \n",
    "- cannot split randomly → ****************************************************************forecasting must not know future****************************************************************\n",
    "    - split by time threshold\n",
    "\n",
    "> **Appropriately split time series data, both train/test split and cross-validation.**\n",
    "> \n",
    "\n",
    "```python\n",
    "rain_df[\"Date\"].min()\n",
    "# Timestamp('2007-11-01 00:00:00')\n",
    "rain_df[\"Date\"].max()\n",
    "# Timestamp('2017-06-25 00:00:00')\n",
    "\n",
    "# then split by time in the middle\n",
    "```\n",
    "\n",
    "`TimeSeriesSplit` - sort dataframe by date for ********************************cross-validation********************************\n",
    "\n",
    "```python\n",
    "lr_pipe = make_pipeline(\n",
    "\tpreprocessor, \n",
    "\tLogisticRegression(max_iter=1000)\n",
    ")\n",
    "cross_val_score(\n",
    "\tlr_pipe, train_df_ordered, y_train_ordered, \n",
    "\tcv=TimeSeriesSplit()).mean()\n",
    ")\n",
    "```\n",
    "\n",
    "> **Perform time series feature engineering:**\n",
    "> \n",
    "\n",
    "> **Encode time as various features in a tabular dataset**\n",
    "> \n",
    "\n",
    "```python\n",
    "# Encodes days as number\n",
    "first_day = train_df[\"Date\"].min()\n",
    "\n",
    "train_df = train_df.assign(\n",
    "    Days_since=train_df[\"Date\"].apply(lambda x: (x - first_day).days)\n",
    ")\n",
    "# OHE of month\n",
    "train_df = train_df.assign(\n",
    "    Month=train_df[\"Date\"].apply(lambda x: x.month_name())\n",
    ")\n",
    "```\n",
    "\n",
    "> **Create lag-based features**\n",
    "> \n",
    "\n",
    "```python\n",
    "orig_feature = \"Rainfall\"\n",
    "lag = -1\n",
    "new_df = df.copy()\n",
    "new_feature_name = f\"{orig_feature}_lag{lag}\"\n",
    "\n",
    "# if there are multiple time series by category\n",
    "for location, df_location in new_df.groupby(\n",
    "        \"Location\"\n",
    "):  # Each location is its own time series\n",
    "  new_df.loc[df_location.index[-lag:], new_feature_name] = df_location.iloc[:lag][\n",
    "      orig_feature\n",
    "  ].values\n",
    "\n",
    "```\n",
    "\n",
    "> **Explain how can you forecast multiple time steps into the future.**\n",
    "> \n",
    "- Approaches to predict into future\n",
    "    1. Train separate model for ******************each time step******************\n",
    "        - e.g. predict next month, predict two months\n",
    "    2. multi-output model jointly predicting multiple time steps\n",
    "    3. One model predicts one time step, and uses it to predict next time step… `for` loop\n",
    "\n",
    "> **Explain the challenges of time series data with unequally spaced time points.**\n",
    "> \n",
    "- if unequally spaced\n",
    "    - can still do feature engineering\n",
    "    - lags would not make sense\n",
    "        - could group into equal bins\n",
    "\n",
    "> **At a high level, explain the concept of trend.**\n",
    "> \n",
    "- patterns that emerge over continuous time\n",
    "    - usually consistent upward/downward movement"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 19 - Survival Analysis\n",
    "\n",
    "> **Explain what is right-censored data.**\n",
    "> \n",
    "- his method suffers from *right-censoring* in which the method is biased towards the window of data collection\n",
    "    - data that isn’t fully captured within the window is biased as it is incomplete\n",
    "\n",
    "> **Explain the problem with treating right-censored data the same as “regular” data.**\n",
    "> \n",
    "- Not all the data points are complete, the end of data capture window is a cut-off\n",
    "    - data will tend to under-estimate\n",
    "\n",
    "> **Determine whether survival analysis is an appropriate tool for a given problem.**\n",
    "> \n",
    "- helps answer questions such as\n",
    "    1. how long customers stay\n",
    "    2. for customer, predict how long they may stay\n",
    "    3. factors influencing churn time\n",
    "\n",
    "> **Apply survival analysis in Python using the `lifelines` package.**\n",
    "> \n",
    "> \n",
    "> > **Interpret a survival curve, such as the Kaplan-Meier curve.**\n",
    "> > \n",
    "\n",
    "```python\n",
    "kmf = lifelines.KaplanMeierFitter()\n",
    "kmf.fit(train_df_surv[\"tenure\"], train_df_surv[\"Churn\"])\n",
    "kmf.survival_function_.plot();\n",
    "# or with error \n",
    "# kmf.plot();\n",
    "plt.title(\"Survival function of customer churn\")\n",
    "plt.xlabel(\"Time with service (months)\")\n",
    "plt.ylabel(\"Survival probability\");\n",
    "```\n",
    "\n",
    "- can look at KM curves for different groups\n",
    "    - e.g. market segments\n",
    "\n",
    "> **Interpret the coefficients of a fitted Cox proportional hazards model.**\n",
    "> \n",
    "- ************************************************************Cox proportional hazard model:************************************************************ interpret how features influence censor duration\n",
    "    - ****coefficient**** for each feature → influence on survival\n",
    "    - **************************************************************proportional hazards assumption**************************************************************\n",
    "\n",
    "```python\n",
    "cph = lifelines.CoxPHFitter(penalizer=0.1)\n",
    "cph.fit(train_df_surv, duration_col=\"tenure\", event_col=\"Churn\");\n",
    "\n",
    "cph_params = pd.DataFrame(cph.params_).sort_values(by=\"coef\", ascending=False)\n",
    "cph_params\n",
    "\n",
    "cph.summary\n",
    "\n",
    "# confidence intervals\n",
    "cph.plot();\n",
    "```\n",
    "\n",
    "> **Make predictions for existing individuals and interpret these predictions.**\n",
    "> \n",
    "\n",
    "```python\n",
    "cph.predict_expectation(test_df_surv)\n",
    "\n",
    "# survival function for individuals\n",
    "cph.predict_survival_function(test_df_surv).plot()\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20 - Ethics\n",
    "\n",
    "> **Sources of bias**\n",
    "> \n",
    "- ************Historical Bias:************ training data reflects biases/prejudices from past\n",
    "- **********************************Measurement Bias:********************************** data not accurately measured for intention\n",
    "- ****************************************Representation bias:**************************************** data doesn’t accurately represent population/phenomenon of interest\n",
    "\n",
    "> **Why algorithmic bias matter and how can it impacts us**\n",
    "> \n",
    "- marginalized groups\n",
    "- can emphasize existing biases/stereotypes/prejudice\n",
    "\n",
    "> **Different fairness metrics**\n",
    "> \n",
    "- ************************************demographic/statistical parity:************************************ population percentage reflected in output classes\n",
    "- **************************************Equality of False Negatives or equalized odds:************************************** constant-false-negative rates across groups\n",
    "- ************************************Equal opportunity:************************************ equal True Positive Rate for all groups"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 21 - Communication\n",
    "\n",
    "> **When communicating about applied ML, tailor an explanation to the intended audience.**\n",
    "> \n",
    "\n",
    "> **Apply best practices of technical communication, such as bottom-up explanations and reader-centric writing.**\n",
    "> \n",
    "\n",
    "> **Given an ML problem, analyze the decision being made and the objectives.**\n",
    "> \n",
    "\n",
    "> **Avoid the pitfall of thinking about ML as coding in isolation; build the habit of relating your work to the surrounding context and stakeholders.**\n",
    "> \n",
    "\n",
    "> **Interpret a confidence score or credence, e.g. what does it mean to be 5% confident that a statement is true.**\n",
    "> \n",
    "- ****************************************credence in practice****************************************\n",
    "    1. ********************************************************************I would accept a bet at these odds********************************************************************\n",
    "        - e.g. 99% → to win 1, I would bet 99\n",
    "        - e.g. 75% → to win 25, I would bet 75\n",
    "    2. ********Long-run frequency of correctness********\n",
    "        - e.g. 99% → for every 100 predictions, I expect 1 to be incorrect\n",
    "        - e.g. 75% → for every 100 predictions, I expect 25 to be incorrect\n",
    "\n",
    "> **Maintain a healthy skepticism of `predict_proba` scores and their possible interpretation as credences.**\n",
    "> \n",
    "\n",
    "> **Be careful and precise when communicating confidence to stakeholders in an ML project.**\n",
    "> \n",
    "\n",
    "> **Identify misleading visualizations.**\n",
    "> \n",
    "- Things to watch out for\n",
    "    - Chopping off the x-axis\n",
    "    - Saturate the axes\n",
    "    - Bar chart for a cherry-picked values\n",
    "    - Different y-axes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
